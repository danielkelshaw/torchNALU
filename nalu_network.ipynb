{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from models import MLP, NAC, NALU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "arithmetic_functions = {\n",
    "    'add': lambda x, y: x + y,\n",
    "    'sub': lambda x, y: x - y,\n",
    "    'mul': lambda x, y: x * y,\n",
    "    'div': lambda x, y: x / y,\n",
    "    'squared': lambda x: torch.pow(x, 2),\n",
    "    'sqrt': lambda x: torch.sqrt(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'None': None,\n",
    "    'NAC': None,\n",
    "    'NALU': None,\n",
    "    'ReLU6': nn.ReLU6(),\n",
    "    'Tanh': nn.Tanh(),\n",
    "    'Sigmoid': nn.Sigmoid(),\n",
    "    'Softsign': nn.Softsign(),\n",
    "    'SELU': nn.SELU(),\n",
    "    'ELU': nn.ELU(),\n",
    "    'ReLU': nn.ReLU()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dim, fn, support):\n",
    "    X = torch.FloatTensor(*dim).uniform_(*support)\n",
    "    y = fn(*[X[:, i] for i in range(dim[1])]).unsqueeze(1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data, target, n_epochs):\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        m = torch.mean(torch.abs(target - output))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch {:05}:\\t'\n",
    "                  'Loss = {:.5f}\\t'\n",
    "                  'MEA = {:.5f}'.format(epoch, loss, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data, target):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        m = torch.mean(torch.abs(target - output))\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = 2\n",
    "n_layers = 2\n",
    "\n",
    "interp_support = [1, 100]\n",
    "extrap_support = [101, 200]\n",
    "\n",
    "n_epochs = 10_000\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for fn_type, fn in arithmetic_functions.items():\n",
    "\n",
    "    if fn_type in ['squared', 'sqrt']:\n",
    "        in_dim = 1\n",
    "    else:\n",
    "        in_dim = 2\n",
    "\n",
    "    print('-> Testing function: {}'.format(fn_type))\n",
    "\n",
    "    Xtrain, ytrain = generate_data(\n",
    "        dim=(500, in_dim), fn=fn, support=interp_support\n",
    "    )\n",
    "\n",
    "    Xtest_interp, ytest_interp = generate_data(\n",
    "        dim=(50, in_dim), fn=fn, support=interp_support\n",
    "    )\n",
    "\n",
    "    Xtest_extrap, ytest_extrap = generate_data(\n",
    "        dim=(50, in_dim), fn=fn, support=extrap_support\n",
    "    )\n",
    "\n",
    "    print('-> Training random.')\n",
    "    net = MLP(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=1, n_layers=n_layers, act=None)\n",
    "    \n",
    "    random_mse_interp = torch.mean(torch.stack([test(net, Xtest_interp, ytest_interp) for i in range(100)])).item()\n",
    "    random_mse_extrap = torch.mean(torch.stack([test(net, Xtest_extrap, ytest_extrap) for i in range(100)])).item()\n",
    "\n",
    "    for name, model in models.items():\n",
    "\n",
    "        if name == 'NAC':\n",
    "            net = NAC(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=1, n_layers=n_layers)\n",
    "        elif name == 'NALU':\n",
    "            net = NALU(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=1, n_layers=n_layers)\n",
    "        else:\n",
    "            net = MLP(in_dim=in_dim, hidden_dim=hidden_dim, out_dim=1, n_layers=n_layers, act=model)\n",
    "\n",
    "        print('-> Running: {}'.format(name))\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        train(net, optimizer, criterion, Xtrain, ytrain, n_epochs)\n",
    "\n",
    "        interp_mse = test(net, Xtest_interp, ytest_interp).item()\n",
    "        extrap_mse = test(net, Xtest_extrap, ytest_extrap).item()\n",
    "\n",
    "        _tmp_interp = {\n",
    "            'type': 'interp',\n",
    "            'fn_type': fn_type,\n",
    "            'activation': name,\n",
    "            'mse': interp_mse,\n",
    "            'random_mse': random_mse_interp\n",
    "        }\n",
    "\n",
    "        _tmp_extrap = {\n",
    "            'type': 'extrap',\n",
    "            'fn_type': fn_type,\n",
    "            'activation': name,\n",
    "            'mse': extrap_mse,\n",
    "            'random_mse': random_mse_extrap\n",
    "        }\n",
    "\n",
    "        results.append(_tmp_interp)\n",
    "        results.append(_tmp_extrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results['normalised_mse'] = df_results.apply(lambda row: 100.0 * row['mse'] / row['random_mse'], axis=1)\n",
    "df_results.to_csv('results.csv')\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interp = df_results[df_results['type'] == 'interp']\n",
    "df_extrap = df_results[df_results['type'] == 'extrap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 2, figsize=(20, 20))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, fn in enumerate(df_interp.fn_type.unique()):\n",
    "    sns.barplot(x='activation', y='normalised_mse', data=df_interp[df_interp['fn_type'] == fn], palette='YlOrRd', ax=axs[2 * idx])\n",
    "    sns.barplot(x='activation', y='normalised_mse', data=df_extrap[df_extrap['fn_type'] == fn], palette='YlOrRd', ax=axs[2 * idx + 1])\n",
    "    axs[2 * idx].set_title(f'interp function = {fn}')\n",
    "    axs[2 * idx + 1].set_title(f'extrap function = {fn}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('normalised_mse.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
